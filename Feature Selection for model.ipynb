{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Values\n",
    "\n",
    "In the machine learning workflow, once we've selected the model we want to use, selecting the appropriate features for that model is the next important step. In this mission, we'll explore how to use correlation between features and the target column, correlation between features, and variance of features to select features. We'll continue working with the same housing dataset from the last mission.\n",
    "\n",
    "We'll specifically focus on selecting from feature columns that don't have any missing values or don't need to be transformed to be useful (e.g. columns like Year Built and Year Remod/Add). We'll explore how to deal with both of these in a later mission in this course.\n",
    "\n",
    "To start, let's look at which columns fall into either of these two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order              0\n",
      "MS SubClass        0\n",
      "Lot Area           0\n",
      "Overall Qual       0\n",
      "Overall Cond       0\n",
      "1st Flr SF         0\n",
      "2nd Flr SF         0\n",
      "Low Qual Fin SF    0\n",
      "Gr Liv Area        0\n",
      "Full Bath          0\n",
      "Half Bath          0\n",
      "Bedroom AbvGr      0\n",
      "Kitchen AbvGr      0\n",
      "TotRms AbvGrd      0\n",
      "Fireplaces         0\n",
      "Garage Cars        0\n",
      "Garage Area        0\n",
      "Wood Deck SF       0\n",
      "Open Porch SF      0\n",
      "Enclosed Porch     0\n",
      "3Ssn Porch         0\n",
      "Screen Porch       0\n",
      "Pool Area          0\n",
      "Misc Val           0\n",
      "SalePrice          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('AmesHousing.txt', delimiter=\"\\t\")\n",
    "train = data[0:1460]\n",
    "#train.info()\n",
    "test = data[1460:]\n",
    "numerical_train = train.select_dtypes(include=['int64', 'float64'])\n",
    "numerical_train.head(5)\n",
    "numerical_train=numerical_train.drop(['PID','Year Built','Year Remod/Add','Garage Yr Blt','Mo Sold','Yr Sold'],axis=1)\n",
    "null_series = numerical_train.isnull().sum()\n",
    "#print(null_series)\n",
    "full_cols_series = null_series[null_series == 0] #subset of null_series to keep only the columns with no missing values,\n",
    "print(full_cols_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlating Feature Columns With Target Column\n",
    "\n",
    "In the last mission, we selected the feature for the simple linear regression model by comparing how some of the features correlate with the target column. If you recall, we focused on 4 features in particular and used the pandas.DataFrame.corr() method to return the correlation coefficients between each pairs of columns. This means that the correlation matrix for 4 columns results in 16 correlation values:\n",
    "\n",
    ">>> train[['GarageArea', 'GrLivArea', 'OverallCond', 'SalePrice']].corr()\n",
    "\n",
    "             GarageArea  GrLivArea  OverallCond  SalePrice\n",
    "\n",
    "GarageArea     1.000000   0.468997    -0.151521   0.623431\n",
    "\n",
    "GrLivArea      0.468997   1.000000    -0.079686   0.708624\n",
    "\n",
    "OverallCond   -0.151521  -0.079686     1.000000  -0.077856\n",
    "\n",
    "SalePrice      0.623431   0.708624    -0.077856   1.000000\n",
    "\n",
    "The subset of features we want to focus on, full_cols_series, contains 27 columns:\n",
    "\n",
    ">>> len(full_cols_series)\n",
    "\n",
    "27\n",
    "\n",
    "The resulting correlation matrix will contain 27 * 27 or 729 correlation values. Comparing and contrasting this many values is incredibly difficult. Let's instead focus on just how the feature columns correlate with the target column (SalePrice) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Order', u'MS SubClass', u'Lot Area', u'Overall Qual', u'Overall Cond',\n",
      "       u'1st Flr SF', u'2nd Flr SF', u'Low Qual Fin SF', u'Gr Liv Area',\n",
      "       u'Full Bath', u'Half Bath', u'Bedroom AbvGr', u'Kitchen AbvGr',\n",
      "       u'TotRms AbvGrd', u'Fireplaces', u'Garage Cars', u'Garage Area',\n",
      "       u'Wood Deck SF', u'Open Porch SF', u'Enclosed Porch', u'3Ssn Porch',\n",
      "       u'Screen Porch', u'Pool Area', u'Misc Val', u'SalePrice'],\n",
      "      dtype='object')\n",
      "Misc Val           0.009903\n",
      "3Ssn Porch         0.038699\n",
      "Low Qual Fin SF    0.060352\n",
      "Order              0.068181\n",
      "MS SubClass        0.088504\n",
      "Overall Cond       0.099395\n",
      "Screen Porch       0.100121\n",
      "Bedroom AbvGr      0.106941\n",
      "Kitchen AbvGr      0.130843\n",
      "Pool Area          0.145474\n",
      "Enclosed Porch     0.165873\n",
      "2nd Flr SF         0.202352\n",
      "Half Bath          0.272870\n",
      "Lot Area           0.274730\n",
      "Wood Deck SF       0.319104\n",
      "Open Porch SF      0.344383\n",
      "TotRms AbvGrd      0.483701\n",
      "Fireplaces         0.485683\n",
      "Full Bath          0.518194\n",
      "1st Flr SF         0.657119\n",
      "Garage Area        0.662397\n",
      "Garage Cars        0.663485\n",
      "Gr Liv Area        0.698990\n",
      "Overall Qual       0.804562\n",
      "SalePrice          1.000000\n",
      "Name: SalePrice, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_subset = train[full_cols_series.index]\n",
    "print(full_cols_series.index)\n",
    "#print(train_subset.head(5))\n",
    "corrmat = train_subset.corr()\n",
    "sorted_corrs = corrmat['SalePrice'].abs().sort_values()\n",
    "print(sorted_corrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix Heatmap\n",
    "\n",
    "We now have a decent list of candidate features to use in our model, sorted by how strongly they're correlated with the SalePrice column. For now, let's keep only the features that have a correlation of 0.3 or higher. This cutoff is a bit arbitrary and, in general, it's a good idea to experiment with this cutoff. For example, you can train and test models using the columns selected using different cutoffs and see where your model stops improving.\n",
    "\n",
    "The next thing we need to look for is for potential collinearity between some of these feature columns. Collinearity is when 2 feature columns are highly correlated and stand the risk of duplicating information. If we have 2 features that convey the same information using 2 different measures or metrics, we need to choose just one or predictive accuracy can suffer.\n",
    "\n",
    "While we can check for collinearity between 2 columns using the correlation matrix, we run the risk of information overload. We can instead generate a correlation matrix heatmap using Seaborn to visually compare the correlations and look for problematic pairwise feature correlations. Because we're looking for outlier values in the heatmap, this visual representation is easier.\n",
    "\n",
    "Here's what the example correlation matrix heatmap looks like from the documentation:\n",
    "\n",
    "Correlation Heatmap Matrix\n",
    "\n",
    "To generate a correlation matrix heatmap, we need to pass in the data frame containing the correlation matrix as a data frame into the seaborn.heatmap() function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure(figsize=(10,6))\n",
    "strong_corrs = sorted_corrs[sorted_corrs > 0.3]\n",
    "corrmat = train_subset[strong_corrs.index].corr()\n",
    "#sns.heatmap(corrmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train And Test Model\n",
    "\n",
    "Based on the correlation matrix heatmap, we can tell that the following pairs of columns are strongly correlated:\n",
    "\n",
    "    Gr Liv Area and TotRms AbvGrd\n",
    "    Garage Area and Garage Cars\n",
    "\n",
    "If we read the descriptions of these columns from the data documentation, we can tell that each pair of column reflects very similar information. Bceause Gr Liv Area and Garage Area are continuous variables that capture more nuance, let's drop the TotRms AbvGrd and Garage Cars.\n",
    "\n",
    "The last thing we'll need to do is confirm that the test set contains no missing values for these columns:\n",
    "\n",
    ">>> final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "\n",
    ">>> test[final_corr_cols.index]\n",
    "\n",
    "class 'pandas.core.frame.DataFrame'\n",
    "\n",
    "RangeIndex: 1470 entries, 1460 to 2929\n",
    "\n",
    "Data columns (total 9 columns):\n",
    "\n",
    "Wood Deck SF     1470 non-null int64\n",
    "\n",
    "Open Porch SF    1470 non-null int64\n",
    "\n",
    "Fireplaces       1470 non-null int64\n",
    "\n",
    "Full Bath        1470 non-null int64\n",
    "\n",
    "1st Flr SF       1470 non-null int64\n",
    "\n",
    "Garage Area      1469 non-null float64\n",
    "\n",
    "Gr Liv Area      1470 non-null int64\n",
    "\n",
    "Overall Qual     1470 non-null int64\n",
    "\n",
    "SalePrice        1470 non-null int64\n",
    "\n",
    "dtypes: float64(1), int64(8)\n",
    "\n",
    "memory usage: 103.4 KB\n",
    "\n",
    "Looks like the test set has one pesky row with a missing value for Garage Area. Let's just drop this row for now. Finally, let's train and test a model using these columns to see how they fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34173.9762919\n",
      "41032.0261202\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "final_corr_cols = strong_corrs.drop(['Garage Cars', 'TotRms AbvGrd'])\n",
    "features = final_corr_cols.drop(['SalePrice']).index\n",
    "target = 'SalePrice'\n",
    "\n",
    "clean_test = test[final_corr_cols.index].dropna()\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[features], train['SalePrice'])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train[target])\n",
    "test_mse = mean_squared_error(test_predictions, clean_test[target])\n",
    "\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse)\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Low Variance Features\n",
    "\n",
    "The last technique we'll explore is removing features with low variance. When the values in a feature column have low variance, they don't meaningfully contribute to the model's predictive capability. On the extreme end, let's imagine a column with a variance of 0. This would mean that all of the values in that column were exactly the same. This means that the column isn't informative and isn't going to help the model make better predictions.\n",
    "\n",
    "To make apples to apples comparisions between columns, we need to standardize all of the columns to vary between 0 and 1. Then, we can set a cutoff value for variance and remove features that have less than that variance amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open Porch SF    0.013938\n",
      "Gr Liv Area      0.018014\n",
      "Full Bath        0.018621\n",
      "1st Flr SF       0.019182\n",
      "Overall Qual     0.019842\n",
      "Garage Area      0.020347\n",
      "Wood Deck SF     0.033064\n",
      "Fireplaces       0.046589\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "unit_train = train[features]/(train[features].max())\n",
    "sorted_vars = unit_train.var().sort_values()\n",
    "print(sorted_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model\n",
    "\n",
    "To wrap up this mission, let's set a cutoff variance of 0.015, remove the Open Porch SF feature, and train and test a model using the remaining features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34372.6967078\n",
      "40591.4270244\n"
     ]
    }
   ],
   "source": [
    "features = features.drop(['Open Porch SF'])\n",
    "\n",
    "clean_test = test[final_corr_cols.index].dropna()\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(train[features], train['SalePrice'])\n",
    "\n",
    "train_predictions = lr.predict(train[features])\n",
    "test_predictions = lr.predict(clean_test[features])\n",
    "\n",
    "train_mse = mean_squared_error(train_predictions, train[target])\n",
    "test_mse = mean_squared_error(test_predictions, clean_test[target])\n",
    "\n",
    "train_rmse_2 = np.sqrt(train_mse)\n",
    "test_rmse_2 = np.sqrt(test_mse)\n",
    "\n",
    "print(train_rmse_2)\n",
    "print(test_rmse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "\n",
    "We were able to improve the RMSE value to approximately 40591 by removing the Open Porch SF feature. This is most likely the furthest we can go without transforming and utilizing the other features in the dataset so we'll stop here for now. In the next 2 missions, we'll explore 2 different ways of fitting models. Afterwards, we'll explore ways to clean and engineer new features from the existing features to improve model accuracy even further."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
